{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from spacy.en import English\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def StringToFunction(func_name):\n",
    "    possibles = globals().copy()\n",
    "    possibles.update(locals())\n",
    "    func = possibles.get(func_name)\n",
    "    if not func:\n",
    "        raise NotImplementedError(\"Function %s not implemented\" % func_name)\n",
    "    return func\n",
    "\n",
    "class JobScraper(object):\n",
    "    def __init__(self, spider, parmdata):\n",
    "        self.spider = StringToFunction(spider)\n",
    "        self.parmdata = parmdata\n",
    "        \n",
    "    def Spider(self,*argv):\n",
    "        jobs = []\n",
    "        nlp = English()\n",
    "        self.data = list()\n",
    "        for joburl in self.spider(self,*argv):\n",
    "            self.data.append(dict())\n",
    "            r = requests.get(joburl, cookies=self.cookies)\n",
    "            soup = BeautifulSoup(r.text,\"lxml\")\n",
    "            content = soup.body.find('div', attrs={'id':'ctl00_MainContent_PrimaryContent'})\n",
    "            jobs.append(dict())\n",
    "            for span in content.span.find_all('span',recursive=True):\n",
    "                  if span.has_attr('aria-labelledby'):\n",
    "                        #print span['id'] + \" : \" + span.text\n",
    "                        self.data[-1][span['id']] = span.getText(separator=u' ')\n",
    "                        #doc = nlp(span.text)\n",
    "                        #for np in doc.noun_chunks:\n",
    "                        #    np.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JobScraper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cff7da07b537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mparmdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'partnerid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m54\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'siteid'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m5346\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'site'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'https://xjobs.brassring.com/TGWebHost'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJobScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BrassRingSpider'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparmdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'JobScraper' is not defined"
     ]
    }
   ],
   "source": [
    "def NullSpider(url):\n",
    "    yield url\n",
    "    \n",
    "def DullSpider(url):\n",
    "    yield url + 'dull'\n",
    "    \n",
    "def BrassRingSpider(self):\n",
    "    # we need to get the cookies from this \"base url\" to make subsequent queries\n",
    "    self.parmdata['baseurl'] = \"%s/searchopenings.aspx?partnerid=%s&siteid=%s\"%(self.parmdata['site'],self.parmdata['partnerid'],self.parmdata['siteid'])\n",
    "    baser = requests.get(self.parmdata['baseurl'])\n",
    "    self.cookies = baser.cookies\n",
    "    \n",
    "    # identify the url for performing search queries\n",
    "    searchpage = dict()\n",
    "    soup = BeautifulSoup(baser.text,\"lxml\")\n",
    "    search = soup.find_all('form',attrs={'id' : 'aspnetForm'})\n",
    "    \n",
    "    # loop over search result pages; each page contains 50 listings by default\n",
    "    startrecord = 1\n",
    "    while True:\n",
    "        searchpage['url'] = \"%s/%s\"%(self.parmdata['site'],search[0]['action'])\n",
    "        headers = {'recordstart':startrecord}\n",
    "        searchpage['data'] = requests.post(searchpage['url'],cookies=baser.cookies,data=headers)\n",
    "        searchpage['soup'] = BeautifulSoup(unicode(searchpage['data'].text),\"lxml\")\n",
    "        for maincontent in searchpage['soup'].find_all('input',attrs={'id':'ctl00_MainContent_GridFormatter_json_tabledata'}):\n",
    "            if maincontent.has_attr('id'):\n",
    "                subsoup = BeautifulSoup(maincontent['value'],\"lxml\")\n",
    "                for job in subsoup.find_all('input',attrs={'name':'chkJobClientIds'}):\n",
    "                    jobid = job['id']\n",
    "                    joburl = \"%s/jobdetails.aspx?jobId=%s&JobSiteId=%s\"%(self.parmdata['site'],jobid,self.parmdata['siteid'])\n",
    "                    yield joburl\n",
    "                    \n",
    "        numjobs = int(searchpage['soup'].find('input',{'name':'totalrecords'})['value'])\n",
    "        if startrecord == 1:\n",
    "            bar = tqdm_notebook(total=numjobs)\n",
    "        bar.update(50)\n",
    "        if startrecord + 50 >= numjobs:\n",
    "            break\n",
    "        else:\n",
    "            startrecord = startrecord + 50\n",
    "\n",
    "parmdata={'partnerid':54,'siteid' : 5346, 'site':'https://xjobs.brassring.com/TGWebHost'}\n",
    "\n",
    "test = JobScraper('BrassRingSpider',parmdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mySpider = test.Spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bleh = unicode(\"\\uI'm the fat dog!\")\n",
    "nlp = English()\n",
    "doc = nlp(bleh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Job Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-6939bd0f352c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Job Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mblah\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Job Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnoun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblah\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mnoun\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Job Title'"
     ]
    }
   ],
   "source": [
    "for data in test.data:\n",
    "    print data['Job Title']\n",
    "    blah = nlp(data['Job Description'])\n",
    "    for noun in blah.noun_chunks:\n",
    "        print noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business': u'GE Renewable Energy',\n",
       " 'Business Segment': u'REN-ONW OnShore Wind',\n",
       " 'Career Level': u'Entry-Level',\n",
       " 'City': u'Bangalore',\n",
       " 'Desired Characteristics': u'\\u2022Knowledge in the field of Wind Turbine Technology. \\u2022Programming skills to develop tools for process automations (Excel VBA, etc.) \\u2022Ability to work with and across all global resources (US, Europe, India, China).',\n",
       " 'Essential Responsibilities': u'\\u2022Support NTI/NPI IP clearance programs by collaborating with technical experts, program managers & IP counsel. \\u2022Explore IP landscaping opportunities and lead the initiative by performing patent analysis. Work with technical experts on white space  opportunities to come up with Invention Disclosure Letters (IDLs). \\u2022Work with domain experts and IP counsel on performing patent invalidity studies.  \\u2022Support patent evaluation board on IDL reviews and provide inputs to bring quality inventions. \\u2022Periodically summarize granted patents across the globe and send alerts to relevant domain experts to work on IP concerns. \\u2022Analyze recently published patents / utility models in various countries and release report monthly to relevant stakeholders. \\u2022Support IP culture drive in BEC Renewable engineering team through IP workouts, tech trend sessions etc..  \\u2022Support inventors in IDL submissions, prior arts searches & office actions.',\n",
       " 'Function': u'Engineering/Technology',\n",
       " 'Function Segment': u'Research',\n",
       " 'Job Number': u'2406375',\n",
       " 'Location(s) Where Opening Is Available': u'India',\n",
       " 'Postal Code': u'560066',\n",
       " 'Posted Position Title': u'Engineer, Intellectual Property',\n",
       " 'Qualifications/Requirements': u'\\u2022Master\\u2019s degree in Electrical, Electrical & Electronics Engineering with \\u2022Good knowledge on usage of patent search tools and databases like Thomson Innovation, PatBase, US PTO, Espacenet, JPO, WIPO etc. \\u2022Good knowledge on various IP studies like Prior-art searches, Landscape analysis, patent invalidity and IP clearance. \\u2022Good knowledge of USA, EU & Indian patent laws. \\u2022Broad technical knowledge and understanding, ability to understand technical content quickly and to categorize it. \\u2022Should possess good oral and written communication skills, with very good text comprehension skills. Affinity to written explanation of technical matters.',\n",
       " 'Relocation Assistance': u'No'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
